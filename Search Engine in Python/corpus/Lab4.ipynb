{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import heapq\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "tokens = []\n",
    "corpus = []\n",
    "wordfreq = {}\n",
    "word_idf_values = {}\n",
    "word_tf_values = {}\n",
    "tfidf_values = []\n",
    "tfidf = {}\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataRetrival():\n",
    "    global data, tokens, corpus, wordfreq, word_idf_values, word_tf_values, tfidf_values, tfidf, current_path\n",
    "    ans = []\n",
    "    for file_name in os.listdir(current_path):\n",
    "        if file_name.endswith('txt'):\n",
    "            file = open(file_name, 'r')\n",
    "            ans.append(file.read())\n",
    "            file.close()\n",
    "    data = \"\".join(i for i in ans)\n",
    "    print(\"Data Retrieved\")\n",
    "\n",
    "def RemovePunct():\n",
    "    global data, tokens, corpus, wordfreq, word_idf_values, word_tf_values, tfidf_values, tfidf, current_path\n",
    "    data = \"\".join([char for char in data if char not in string.punctuation])\n",
    "    print()\n",
    "    print(\"Punctuations Removed\")\n",
    "\n",
    "def Tokenization():\n",
    "    global data, tokens, corpus, wordfreq, word_idf_values, word_tf_values, tfidf_values, tfidf, current_path\n",
    "    tokens = word_tokenize(data)\n",
    "    print()\n",
    "    print(\"Tokens Made\")\n",
    "    \n",
    "def RemoveStopWords():\n",
    "    global data, tokens, corpus, wordfreq, word_idf_values, word_tf_values, tfidf_values, tfidf, current_path\n",
    "    tokens = [word for word in tokens if not word in stopwords.words()]\n",
    "    print()\n",
    "    print(\"Stop words removed\")\n",
    "    \n",
    "def Stemming():\n",
    "    global data, tokens, corpus, wordfreq, word_idf_values, word_tf_values, tfidf_values, tfidf, current_path\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    print()\n",
    "    print(\"Stemming done\")\n",
    "    \n",
    "def FrequencyCounting():\n",
    "    global data, tokens, corpus, wordfreq, word_idf_values, word_tf_values, tfidf_values, tfidf, current_path\n",
    "    for file_name in os.listdir(current_path):\n",
    "        if file_name.endswith('txt'):\n",
    "            print(file_name)\n",
    "            file = open(file_name,'r')\n",
    "            temp_data = file.read()\n",
    "            temp_tokens = nltk.word_tokenize(temp_data)\n",
    "            temp_tokens = [word for word in temp_tokens if not word in nltk.corpus.stopwords.words()]\n",
    "            stemmer = PorterStemmer()\n",
    "            temp_tokens = [stemmer.stem(word) for word in temp_tokens]\n",
    "            for temp_token in temp_tokens:\n",
    "                if temp_token in tokens and temp_token not in wordfreq.keys():\n",
    "                    wordfreq[temp_token] = 1\n",
    "                elif temp_token in tokens:\n",
    "                    wordfreq[temp_token] += 1\n",
    "            file.close()\n",
    "    print()\n",
    "    print(\"Frequency Counted\")\n",
    "    \n",
    "def TF_and_IDF():\n",
    "    global data, tokens, corpus, wordfreq, word_idf_values, word_tf_values, tfidf_values, tfidf, current_path\n",
    "    for word in wordfreq:\n",
    "        count = 0\n",
    "        tf_vector=[]\n",
    "        for file_name in os.listdir(current_path):\n",
    "            freq = 0\n",
    "            if file_name.endswith('txt'):\n",
    "                file = open(file_name, 'r')\n",
    "                temp_data = file.read()\n",
    "                temp_tokens = nltk.word_tokenize(temp_data)\n",
    "                temp_tokens = [word for word in temp_tokens if not word in nltk.corpus.stopwords.words()]\n",
    "                stemmer = PorterStemmer()\n",
    "                temp_tokens = [stemmer.stem(word) for word in temp_tokens]\n",
    "                if word in temp_tokens:\n",
    "                    count += 1\n",
    "                for w in temp_tokens:\n",
    "                    if word == w:\n",
    "                        freq += 1\n",
    "                word_tf = freq / len(temp_tokens)\n",
    "                tf_vector.append(word_tf)\n",
    "                file.close()\n",
    "        word_idf_values[word] = np.log(10 / (1 + count))\n",
    "        word_tf_values[word] = tf_vector\n",
    "    print()\n",
    "    print(\"TF and IDF Calculated\")\n",
    "    \n",
    "# def TF():\n",
    "#     global data, tokens, corpus, wordfreq, word_idf_values, word_tf_values, tfidf_values, tfidf, current_path\n",
    "#     for word in wordfreq:\n",
    "#         tf_vector=[]\n",
    "#         for file_name in os.listdir(current_path):\n",
    "#             if file_name.endswith('txt'):\n",
    "#                 freq = 0\n",
    "#                 file = open(file_name, 'r')\n",
    "#                 temp_data = file.read()\n",
    "#                 temp_tokens = nltk.word_tokenize(temp_data)\n",
    "#                 temp_tokens = [word for word in temp_tokens if not word in nltk.corpus.stopwords.words()]\n",
    "#                 stemmer = PorterStemmer()\n",
    "#                 temp_tokens = [stemmer.stem(word) for word in temp_tokens]\n",
    "#                 for w in temp_tokens:\n",
    "#                     if word == w:\n",
    "#                         freq += 1\n",
    "#                 word_tf = freq / len(temp_tokens)\n",
    "#                 tf_vector.append(word_tf)\n",
    "#                 file.close()\n",
    "#         word_tf_values[word] = tf_vector\n",
    "#     print()\n",
    "#     print(\"TF Calculated\")\n",
    "\n",
    "def TF_IDF():\n",
    "    global data, tokens, corpus, wordfreq, word_idf_values, word_tf_values, tfidf_values, tfidf, current_path\n",
    "    i = 0\n",
    "    for word in word_tf_values.keys():\n",
    "        temp_tfidf=[]\n",
    "        for sen in word_tf_values[word]:\n",
    "            score = sen * word_idf_values[word]\n",
    "            temp_tfidf.append(score)\n",
    "        tfidf_values.append(temp_tfidf)\n",
    "        tfidf[word] = i\n",
    "        i += 1\n",
    "    print()\n",
    "    print(\"TF-IDF Calculated\")\n",
    "\n",
    "def preprocessing():\n",
    "    json_data = {}\n",
    "    \n",
    "    # Reading the data\n",
    "    DataRetrival()\n",
    "    \n",
    "    data.lower()\n",
    "    print()\n",
    "    print(\"Data Lowercased\")\n",
    "    \n",
    "    # Removing punctuations.\n",
    "    RemovePunct()\n",
    "\n",
    "    # Doing tokenization\n",
    "    Tokenization()\n",
    "\n",
    "    # Stop word removal\n",
    "    RemoveStopWords()\n",
    "    \n",
    "    # Stemming\n",
    "    Stemming()\n",
    "\n",
    "    # Building Corpus\n",
    "    corpus = tokens\n",
    "    print()\n",
    "    print(\"Corpus ready\")\n",
    "\n",
    "    # Counting frequancy of words in tokens\n",
    "    FrequencyCounting()\n",
    "                    \n",
    "    # Calculating TF and IDF values\n",
    "    TF_and_IDF()\n",
    "\n",
    "    # Calculating tf values\n",
    "    # TF()    \n",
    "    \n",
    "    # Calculating tf-idf value\n",
    "    TF_IDF()\n",
    "    \n",
    "    json_data['preprocessing'] = {}\n",
    "    json_data['preprocessing']['data'] = data\n",
    "    json_data['preprocessing']['tokens'] = tokens\n",
    "    json_data['preprocessing']['corpus'] = corpus\n",
    "    json_data['preprocessing']['wordfreq'] = wordfreq\n",
    "    json_data['preprocessing']['word_idf_values'] = word_idf_values\n",
    "    json_data['preprocessing']['word_tf_values'] = word_tf_values\n",
    "    json_data['preprocessing']['tfidf_values'] = tfidf_values\n",
    "    json_data['preprocessing']['tfidf'] = tfidf\n",
    "    with open('json_data.json', 'w') as outfile:\n",
    "        json.dump(json_data, outfile)\n",
    "    \n",
    "    print()\n",
    "    print(\"Preprocessing Done\")\n",
    "    print()\n",
    "    print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Retieved\n",
      "\n",
      "Data Lowercased\n",
      "\n",
      "Punctuations Removed\n",
      "\n",
      "Tokens Made\n",
      "\n",
      "Stop words removed\n",
      "\n",
      "Stemming done\n",
      "\n",
      "Corpus ready\n",
      "the_rolling_stones.txt\n",
      "kings_of_leon.txt\n",
      "coldplay.txt\n",
      "imagine_dragons.txt\n",
      "the_beatles.txt\n",
      "one_republic.txt\n",
      "green_day.txt\n",
      "nirvana.txt\n",
      "linkin_park.txt\n",
      "maroon_5.txt\n",
      "\n",
      "Frequency Counted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cf1287f81991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c1c8aba7e00b>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m# Calculating TF and IDF values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mTF_and_IDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# Calculating tf values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c1c8aba7e00b>\u001b[0m in \u001b[0;36mTF_and_IDF\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mtemp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mtemp_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mtemp_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mtemp_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c1c8aba7e00b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mtemp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mtemp_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mtemp_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mtemp_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     21\u001b[0m         return [\n\u001b[1;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         ]\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblanklines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"discard\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLineTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblanklines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import ttk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search():\n",
    "    canvas.delete(\"all\")\n",
    "    infile = open(\"json_data.json\", 'r')\n",
    "    json_data = json.load(infile)\n",
    "    infile.close()\n",
    "    data = json_data['preprocessing']['data']\n",
    "    tokens = json_data['preprocessing']['tokens']\n",
    "    corpus = json_data['preprocessing']['corpus']\n",
    "    wordfreq = json_data['preprocessing']['wordfreq']\n",
    "    word_idf_values = json_data['preprocessing']['word_idf_values']\n",
    "    word_tf_values = json_data['preprocessing']['word_tf_values']\n",
    "    tfidf_values = json_data['preprocessing']['tfidf_values']\n",
    "    tfidf = json_data['preprocessing']['tfidf']\n",
    "    \n",
    "    tf_idf_model = np.asarray(tfidf_values)\n",
    "    tf_idf_model = np.transpose(tf_idf_model)\n",
    "    det = []\n",
    "    for row in tf_idf_model:\n",
    "        sum = 0\n",
    "        for num in row:\n",
    "            sum += num ** 2\n",
    "        sum = math.sqrt(sum)\n",
    "        det.append(sum)\n",
    "    s = searchEntry.get()\n",
    "    s.lower()\n",
    "    s = nltk.word_tokenize(s)\n",
    "    s = [word for word in s if not word in nltk.corpus.stopwords.words()]\n",
    "    stemmer = PorterStemmer()\n",
    "    s = [stemmer.stem(word) for word in s]\n",
    "    l = len(s)\n",
    "    arr = []\n",
    "    dic = {}\n",
    "    for i in range(tf_idf_model.shape[0]):\n",
    "        sum = 0.0\n",
    "        for word in s:\n",
    "            index = 0\n",
    "            if word in tfidf.keys():\n",
    "                index = tfidf[word]\n",
    "                sum += tf_idf_model[i][index]\n",
    "        arr.append(sum / (det[i] * math.sqrt(l)))\n",
    "        dic[i] = sum / (det[i] * math.sqrt(l))\n",
    "    heapq.heapify(arr)\n",
    "    ans=[]\n",
    "    ans=heapq.nlargest(10,arr)\n",
    "    print(ans)\n",
    "    print_text = \"\"\n",
    "    for i in ans:\n",
    "        id = 0\n",
    "        for ind, val in dic.items():\n",
    "            if val == i:\n",
    "                id = ind + 1\n",
    "                break\n",
    "        print_text += (\"Document: \" + str(id) + \"\\n\")\n",
    "    canvas.create_text(40, 90, anchor=W, font=\"Ubuntu\", text=\"Search Results: \\n\\n\" + print_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-8-a1b162080354>\", line 3, in search\n",
      "    infile = open(\"json_data.json\", 'r')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'json_data.json'\n"
     ]
    }
   ],
   "source": [
    "root = Tk()\n",
    "root.title(\"Harsh's Search Engine\")\n",
    "root.maxsize(900,600)\n",
    "root.config(bg=\"white\")\n",
    "\n",
    "UI_frame = Frame(root, width=600, height=200, bg='white')\n",
    "UI_frame.grid(row=0, column=0, padx=10, pady=5)\n",
    "\n",
    "canvas = Canvas(root, width=1200, height=760, bg=\"ghost white\")\n",
    "canvas.grid(row=1, column=0, padx=10, pady=5)\n",
    "\n",
    "searchEntry = Entry(UI_frame)\n",
    "searchEntry.grid(row=1, column=1, padx=5, pady=5, sticky=W)\n",
    "Button(UI_frame, text='Click to search',command = search, bg='coral').grid(row=1,column=2,padx=5,pady=5)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
